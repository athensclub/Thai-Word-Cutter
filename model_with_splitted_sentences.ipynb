{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/athensclub/Thai-Word-Cutter/blob/master/main_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_sVizY7dPOB",
        "colab_type": "text"
      },
      "source": [
        "# **Importing and Housekeeping**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQazv8NEPoSQ",
        "colab_type": "code",
        "outputId": "dd67eab8-96b7-4a6f-d3f1-48acb6110511",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from google.colab import files\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Concatenate, Flatten,LSTM\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0c3c2939-30ee-47c9-b61f-360b04d553a9\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-0c3c2939-30ee-47c9-b61f-360b04d553a9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving news_00001.txt to news_00001.txt\n",
            "Saving news_00002.txt to news_00002.txt\n",
            "Saving news_00003.txt to news_00003.txt\n",
            "Saving news_00004.txt to news_00004.txt\n",
            "Saving news_00005.txt to news_00005.txt\n",
            "Saving news_00006.txt to news_00006.txt\n",
            "Saving news_00007.txt to news_00007.txt\n",
            "Saving news_00008.txt to news_00008.txt\n",
            "Saving news_00009.txt to news_00009.txt\n",
            "Saving news_00010.txt to news_00010.txt\n",
            "Saving news_00011.txt to news_00011.txt\n",
            "Saving news_00012.txt to news_00012.txt\n",
            "Saving news_00013.txt to news_00013.txt\n",
            "Saving news_00014.txt to news_00014.txt\n",
            "Saving news_00015.txt to news_00015.txt\n",
            "Saving news_00016.txt to news_00016.txt\n",
            "Saving news_00017.txt to news_00017.txt\n",
            "Saving news_00018.txt to news_00018.txt\n",
            "Saving news_00019.txt to news_00019.txt\n",
            "Saving news_00020.txt to news_00020.txt\n",
            "Saving news_00021.txt to news_00021.txt\n",
            "Saving news_00022.txt to news_00022.txt\n",
            "Saving news_00023.txt to news_00023.txt\n",
            "Saving news_00024.txt to news_00024.txt\n",
            "Saving news_00025.txt to news_00025.txt\n",
            "Saving news_00026.txt to news_00026.txt\n",
            "Saving news_00027.txt to news_00027.txt\n",
            "Saving news_00028.txt to news_00028.txt\n",
            "Saving news_00029.txt to news_00029.txt\n",
            "Saving news_00030.txt to news_00030.txt\n",
            "Saving news_00031.txt to news_00031.txt\n",
            "Saving news_00032.txt to news_00032.txt\n",
            "Saving news_00033.txt to news_00033.txt\n",
            "Saving news_00034.txt to news_00034.txt\n",
            "Saving news_00035.txt to news_00035.txt\n",
            "Saving news_00036.txt to news_00036.txt\n",
            "Saving news_00037.txt to news_00037.txt\n",
            "Saving news_00038.txt to news_00038.txt\n",
            "Saving news_00039.txt to news_00039.txt\n",
            "Saving news_00040.txt to news_00040.txt\n",
            "Saving news_00041.txt to news_00041.txt\n",
            "Saving news_00042.txt to news_00042.txt\n",
            "Saving news_00043.txt to news_00043.txt\n",
            "Saving news_00044.txt to news_00044.txt\n",
            "Saving news_00045.txt to news_00045.txt\n",
            "Saving news_00046.txt to news_00046.txt\n",
            "Saving news_00047.txt to news_00047.txt\n",
            "Saving news_00048.txt to news_00048.txt\n",
            "Saving news_00049.txt to news_00049.txt\n",
            "Saving news_00050.txt to news_00050.txt\n",
            "Saving news_00051.txt to news_00051.txt\n",
            "Saving news_00052.txt to news_00052.txt\n",
            "Saving news_00053.txt to news_00053.txt\n",
            "Saving news_00054.txt to news_00054.txt\n",
            "Saving news_00055.txt to news_00055.txt\n",
            "Saving news_00056.txt to news_00056.txt\n",
            "Saving news_00057.txt to news_00057.txt\n",
            "Saving news_00058.txt to news_00058.txt\n",
            "Saving news_00059.txt to news_00059.txt\n",
            "Saving news_00060.txt to news_00060.txt\n",
            "Saving news_00061.txt to news_00061.txt\n",
            "Saving news_00062.txt to news_00062.txt\n",
            "Saving news_00063.txt to news_00063.txt\n",
            "Saving news_00064.txt to news_00064.txt\n",
            "Saving news_00065.txt to news_00065.txt\n",
            "Saving news_00066.txt to news_00066.txt\n",
            "Saving news_00067.txt to news_00067.txt\n",
            "Saving news_00068.txt to news_00068.txt\n",
            "Saving news_00069.txt to news_00069.txt\n",
            "Saving news_00070.txt to news_00070.txt\n",
            "Saving news_00071.txt to news_00071.txt\n",
            "Saving news_00072.txt to news_00072.txt\n",
            "Saving news_00073.txt to news_00073.txt\n",
            "Saving news_00074.txt to news_00074.txt\n",
            "Saving news_00075.txt to news_00075.txt\n",
            "Saving news_00076.txt to news_00076.txt\n",
            "Saving news_00077.txt to news_00077.txt\n",
            "Saving news_00078.txt to news_00078.txt\n",
            "Saving news_00079.txt to news_00079.txt\n",
            "Saving news_00080.txt to news_00080.txt\n",
            "Saving news_00081.txt to news_00081.txt\n",
            "Saving news_00082.txt to news_00082.txt\n",
            "Saving news_00083.txt to news_00083.txt\n",
            "Saving news_00084.txt to news_00084.txt\n",
            "Saving news_00085.txt to news_00085.txt\n",
            "Saving news_00086.txt to news_00086.txt\n",
            "Saving news_00087.txt to news_00087.txt\n",
            "Saving news_00088.txt to news_00088.txt\n",
            "Saving news_00089.txt to news_00089.txt\n",
            "Saving news_00090.txt to news_00090.txt\n",
            "Saving news_00091.txt to news_00091.txt\n",
            "Saving news_00092.txt to news_00092.txt\n",
            "Saving news_00093.txt to news_00093.txt\n",
            "Saving news_00094.txt to news_00094.txt\n",
            "Saving news_00095.txt to news_00095.txt\n",
            "Saving news_00096.txt to news_00096.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQh-zzkUjl06",
        "colab_type": "text"
      },
      "source": [
        "# **Create a mapping from a character to an integer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqoLfvapY1x9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "characters = 'กขฃคฅฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรฤลฦวศษสหฬอฮฯะัาำิีึืฺุู฿เแโใไๅๆ็่้๊๋์ํ๎๏๐๑๒๓๔๕๖๗๘๙abcdefghijklmnopqrstuvwxyz\"\\'0123456789,.!?/\\\\:;%()[]{}+_-*@#><=^$& \\t\\n'\n",
        "char_encode = {}\n",
        "char_decode = {}\n",
        "i = 1\n",
        "for c in characters:\n",
        "  char_encode[c] = i;\n",
        "  char_decode[i] = c;\n",
        "  i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCsv1F0AjwNm",
        "colab_type": "text"
      },
      "source": [
        "# **Function: `encode(data)`**\n",
        "\n",
        "> Accepts: (data)\n",
        "*   data: the string to be converted to list of integers.\n",
        "\n",
        "\n",
        "> Returns: (encoded)\n",
        "*   encoded: the list of integers encoded from the given data string\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkBNo0GehRS8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode(data):\n",
        "  encoded = []\n",
        "  data = data.lower()\n",
        "  for c in data:\n",
        "    encoded.append(char_encode[c])\n",
        "  return encoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4_-16tjlerZ",
        "colab_type": "text"
      },
      "source": [
        "# **Function: `decode(data)`**\n",
        "\n",
        "> Accepts: (data)\n",
        "*   data: the list of integers to be converted to string\n",
        "\n",
        "> Returns: (decoded)\n",
        "*   decoded: the string that is decoded from list of integers given from data\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc88P1L-lL1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode(data):\n",
        "  decoded = ''\n",
        "  for c in data:\n",
        "    if c != 0:\n",
        "      decoded = decoded + char_decode[c]\n",
        "  return decoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1NGb8obiad9",
        "colab_type": "text"
      },
      "source": [
        "# **Function: `convert_data(data)`**\n",
        "> Accepts: (data)\n",
        "*   data: raw string read from news file\n",
        "\n",
        "> Returns: (encoded,ans)\n",
        "*  encoded: The string that is created from removing separator '|' from original raw string, which is then encoded into integers by function ```encode(data)```\n",
        "*   ans: The array that is of the same length as combined string and has value be either 0 or 1. The 0 means that the character at that index of combined string should not be cut while 1 means that it should be cut\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roP1XwcWaR3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_data(data):\n",
        "  splitted = data.split('|')\n",
        "  encoded = encode(data.replace('|',''))\n",
        "  ans = np.zeros(len(encoded))\n",
        "  i = 0;\n",
        "  for s in splitted:\n",
        "    if(len(s) > 0):\n",
        "      i += len(s) \n",
        "      ans[i - 1] = 1\n",
        "  return encoded,ans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi2TgMTnmTlP",
        "colab_type": "text"
      },
      "source": [
        "# **Function: `split_data(encoded,ans,length=256)`**\n",
        "A function that split by whitespace and then combine them together into array that the size does not exceed the length and return the result of combining all the combined all of the splitted data into one list, mapped to another answer array by index\n",
        "\n",
        "> Accepts: (encoded,ans,length=256)\n",
        "*   encoded: the encoded data that is going to be splitted\n",
        "*   ans: the array of the answer of the data\n",
        "*   length: the maximum size of each splitted data (default value is 256)\n",
        "\n",
        "> Returns: (splitted,splitted_ans)\n",
        "*   splitted: the list of the splitted data\n",
        "*   splitted_ans: the list of the answer to the splitted data, mapped by index\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfPPFOWLeGF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_data(encoded,ans,length=256):\n",
        "  splitted = []\n",
        "  splitted_ans = []\n",
        "  ans_chunk = []\n",
        "  chunk = []\n",
        "  temp = []\n",
        "  ans_temp = []\n",
        "  for i in range(len(encoded)):\n",
        "    c = encoded[i]\n",
        "    temp.append(c)\n",
        "    ans_temp.append(ans[i])\n",
        "    if c == char_encode[' ']:\n",
        "      if len(temp) > 0:\n",
        "        if len(temp) + len(chunk) < length:\n",
        "          chunk.extend(temp)\n",
        "          ans_chunk.extend(ans_temp)\n",
        "          ans_temp = []\n",
        "          temp = []\n",
        "        else:\n",
        "          splitted.append(chunk)\n",
        "          splitted_ans.append(ans_chunk)\n",
        "          chunk = []\n",
        "          ans_chunk = []\n",
        "          chunk.extend(temp)\n",
        "          ans_chunk.extend(ans_temp)\n",
        "          ans_temp = []\n",
        "          temp = []\n",
        "  #cleaning leftovers\n",
        "  if len(temp) > 0:\n",
        "    if len(temp) + len(chunk) < length:\n",
        "      chunk.extend(temp)\n",
        "      ans_chunk.extend(ans_temp)\n",
        "    else:\n",
        "      splitted.append(chunk)\n",
        "      splitted_ans.append(ans_chunk)\n",
        "      chunk.extend(temp)\n",
        "      ans_chunk.extend(ans_temp)\n",
        "  if len(chunk) > 0:\n",
        "    splitted.append(chunk)\n",
        "    splitted_ans.append(ans_chunk)\n",
        "  return splitted,splitted_ans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDkvaGNBoLeX",
        "colab_type": "text"
      },
      "source": [
        "# **Function: `split_text_data(encoded,length=256)`**\n",
        "Similar to ```split_data(encoded,ans,length=256)``` but does not split the answer data\n",
        "\n",
        "> Accepts: (encoded,length=256)\n",
        "*   encoded: the encoded data that is going to be splitted\n",
        "*   length: the maximum size of each splitted data (default value is 256)\n",
        "\n",
        "> Returns: (splitted)\n",
        "*   splitted: the list of the splitted data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gp7TVJNgn9iJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_text_data(encoded,length=256):\n",
        "  splitted = []\n",
        "  chunk = []\n",
        "  temp = []\n",
        "  for i in range(len(encoded)):\n",
        "    c = encoded[i]\n",
        "    temp.append(c)\n",
        "    if c == char_encode[' ']:\n",
        "      if len(temp) > 0:\n",
        "        if len(temp) + len(chunk) < length:\n",
        "          chunk.extend(temp)\n",
        "          temp = []\n",
        "        else:\n",
        "          splitted.append(chunk)\n",
        "          chunk = []\n",
        "          chunk.extend(temp)\n",
        "          temp = []\n",
        "  #cleaning leftovers\n",
        "  if len(temp) > 0:\n",
        "    if len(temp) + len(chunk) < length:\n",
        "      chunk.extend(temp)\n",
        "    else:\n",
        "      splitted.append(chunk)\n",
        "      chunk.extend(temp)\n",
        "  if len(chunk) > 0:\n",
        "    splitted.append(chunk)\n",
        "  return splitted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghAOM5HKhDuX",
        "colab_type": "text"
      },
      "source": [
        "# **Function: ```create_training_data(splitted,splitted_ans)```**\n",
        "Take in a list of chunks of text and where to cut and turn it into a training data\n",
        "\n",
        "> Accepts: (splitted,splitted_ans)\n",
        "*   splitted: The list of chunk of the text\n",
        "*   splitted_ans: the list of chunk of location to cut the text,mapped to the splitted chunk by index\n",
        "\n",
        "> Returns: (before,current,after,ans) Note that every data returned is mapped to one another by index\n",
        "*   before: the training data for model in 'before' input layer\n",
        "*   current: the training data for model in 'current' input layer\n",
        "*   after: the training data for model in 'after' input layer\n",
        "*   ans: the training data that contains answer for the model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKtml91vgCw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_training_data(splitted,splitted_ans):\n",
        "  before = []\n",
        "  current = []\n",
        "  after = []\n",
        "  ans = []\n",
        "  for i in range(len(splitted)):\n",
        "    chunk = splitted[i]\n",
        "    chunk_ans = splitted_ans[i]\n",
        "    temp = []\n",
        "    chunk_before = []\n",
        "    chunk_after = chunk.copy()\n",
        "    for j in range(len(chunk)):\n",
        "      temp = temp.copy()\n",
        "      temp.append(chunk[j])\n",
        "      chunk_after = chunk_after.copy()\n",
        "      chunk_after.pop(0)\n",
        "      before.append(chunk_before)\n",
        "      current.append(temp)\n",
        "      after.append(chunk_after)\n",
        "      ans.append(chunk_ans[j])\n",
        "      if chunk_ans[j] == 1:\n",
        "        chunk_before = chunk_before.copy()\n",
        "        chunk_before.extend(temp)\n",
        "        temp = []\n",
        "    if len(temp) > 0:\n",
        "      before.append(chunk_before)\n",
        "      current.append(temp)\n",
        "      after.append(chunk_after)\n",
        "      ans.append(chunk_ans[len(chunk_ans)-1])\n",
        "  before = sequence.pad_sequences(before,256)\n",
        "  current = sequence.pad_sequences(current,256)\n",
        "  after = sequence.pad_sequences(after,256)\n",
        "  return before,current,after,np.asarray(ans)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1qtVd28e0sr",
        "colab_type": "text"
      },
      "source": [
        "# **Function: ```train(_model,text)```**\n",
        "Train the given model using the input text that is tokenized, splitting each word by the character '|'\n",
        "\n",
        "> Accepts (_model,text)\n",
        "*   _model: the model that is going to be trained\n",
        "*   text: the raw text data that is tokenized, splitting each word by the character '|'\n",
        "\n",
        "> Returns: None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHYKdsUrXGLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(_model,text):\n",
        "  (train_encoded,train_ans) = convert_data(text)\n",
        "  (train_splitted,train_splitted_ans) = split_data(train_encoded,train_ans)\n",
        "  (train_data_before,train_data_current,train_data_after,train_data_ans) = create_training_data(train_splitted,train_splitted_ans)\n",
        "  _model.fit([train_data_before,train_data_current,train_data_after],train_data_ans)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmdVxVOZgwOU",
        "colab_type": "text"
      },
      "source": [
        "# **Function: ```evaluate(_model,text)```**\n",
        "Simular to ```train(_model,text)``` but instead of using the given text data to train the model, it is used to evaluate the model\n",
        "\n",
        "\n",
        "\n",
        "> Accepts (_model,text)\n",
        "*   _model: the model that is going to be evaluated\n",
        "*   text: the raw text data that is tokenized, splitting each word by the character '|'\n",
        "\n",
        "> Returns: (result)\n",
        "*   result: Result of the evaluation. A list of 2 elements, the first element is loss, and the second element is accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRnnXzLlffc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(_model,text):\n",
        "  (train_encoded,train_ans) = convert_data(text)\n",
        "  (train_splitted,train_splitted_ans) = split_data(train_encoded,train_ans)\n",
        "  (train_data_before,train_data_current,train_data_after,train_data_ans) = create_training_data(train_splitted,train_splitted_ans)\n",
        "  result = _model.evaluate([train_data_before,train_data_current,train_data_after],train_data_ans)\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lbHjJgFnWav",
        "colab_type": "text"
      },
      "source": [
        "# **Create and compile the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJEXKFcJEKcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_chars = len(characters)\n",
        "\n",
        "before_input = Input(shape=(None,), name='before')  \n",
        "current_input = Input(shape=(None,), name='current') \n",
        "after_input = Input(shape=(None,), name='after')  \n",
        "\n",
        "before_features = Embedding(num_chars, 64)(before_input)\n",
        "current_features = Embedding(num_chars, 64)(current_input)\n",
        "after_features = Embedding(num_chars, 64)(after_input)\n",
        "\n",
        "before_features = LSTM(128)(before_features)\n",
        "current_features = LSTM(128)(current_features)\n",
        "after_features = LSTM(128)(after_features)\n",
        "\n",
        "x = Concatenate()([before_features, current_features, after_features])\n",
        "x = Dense(128,activation='relu')(x)\n",
        "out = Dense(1,activation='sigmoid',name='output')(x)\n",
        "\n",
        "model = Model(inputs=[before_input, current_input, after_input],\n",
        "                    outputs=[out])\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=['acc'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xUa44gonbjI",
        "colab_type": "text"
      },
      "source": [
        "# **Train the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qljFfWdIEn9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(80):\n",
        "  file = open('news_{:05d}.txt'.format(i+1))\n",
        "  if file.mode == 'r':\n",
        "    train_text = file.read()\n",
        "  file.close()\n",
        "  train(model,train_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxWmM8ND7SWX",
        "colab_type": "text"
      },
      "source": [
        "# **Evaluate the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAtnO3tn5jmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(80,96):  \n",
        "  file = open('news_{:05d}.txt'.format(i+1))\n",
        "  if file.mode == 'r':\n",
        "    test_text = file.read()\n",
        "  file.close()\n",
        "  print(evaluate(model,test_text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ37G6HK6PBZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqPm8qXHfhiK",
        "colab_type": "text"
      },
      "source": [
        "# **Function: ```tokenize(text)```**\n",
        "Take in a normal string and tokenize it. Each sentence length must be below 256 characters and each sentence must be splitted by space bar\n",
        "\n",
        "> Accepts: (text)\n",
        "*   text: the raw text that is going to be tokenized\n",
        "\n",
        "> Returns: (tokenized)\n",
        "*   tokenized: the result of tokenization, in form of list of strings, where each string is the word tokenized\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2N4sdRVnnsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(text):\n",
        "  tokenized = []\n",
        "  splitted = split_text_data(encode(text))\n",
        "  for chunk in splitted:\n",
        "    temp = []\n",
        "    before = []\n",
        "    after = chunk.copy()\n",
        "    for c in chunk:\n",
        "      temp.append(c)\n",
        "      after.pop(0)\n",
        "      pred = model.predict([sequence.pad_sequences([before],256),sequence.pad_sequences([temp],256),sequence.pad_sequences([after],256)])[0]\n",
        "      if pred > 0.5:\n",
        "        tokenized.append(decode(temp))\n",
        "        before.extend(temp)\n",
        "        temp = []\n",
        "    #cleaning leftovers\n",
        "    if len(temp) > 0:\n",
        "      tokenized.append(decode(temp))\n",
        "  return tokenized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOk07fftp-M-",
        "colab_type": "code",
        "outputId": "26e0ab68-e9f0-4687-e1be-9f496f60a8ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenize('ถ้าผมกินนี่ ผมจะตายไหม')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ถ้า', 'ผม', 'กิน', 'นี่', ' ', 'ผม', 'จะ', 'ตาย', 'ไหม']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    }
  ]
}
